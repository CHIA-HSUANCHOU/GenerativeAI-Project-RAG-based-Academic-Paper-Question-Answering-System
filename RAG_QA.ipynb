{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZIZOJV29QUi"
      },
      "source": [
        "## Install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SgAusZDYhFFG"
      },
      "outputs": [],
      "source": [
        "!pip install langchain_community langchain chromadb rouge-score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUtR1Dbw8RL2"
      },
      "outputs": [],
      "source": [
        "!pip install -U langchain-huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09yj5IMlh6L1"
      },
      "outputs": [],
      "source": [
        "!pip install langchain-groq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "leGkdlneBhA5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import shutil\n",
        "import zipfile\n",
        "import os\n",
        "import torch\n",
        "import difflib\n",
        "import chromadb\n",
        "import json\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.schema import Document\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline, AutoModelForCausalLM, AutoModelForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.schema.retriever import BaseRetriever\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from tqdm import tqdm\n",
        "from operator import itemgetter\n",
        "from langchain.chains import LLMChain\n",
        "from rouge_score import rouge_scorer\n",
        "from pprint import pprint\n",
        "from collections import Counter\n",
        "from huggingface_hub import hf_hub_download\n",
        "from typing import List\n",
        "from tqdm import tqdm\n",
        "from json import dumps, loads\n",
        "from transformers import pipeline\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from sentence_transformers import CrossEncoder\n",
        "from huggingface_hub import HfApi, HfFolder, upload_file\n",
        "from huggingface_hub import login\n",
        "from langchain_groq import ChatGroq\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ta7fg6abvGIX"
      },
      "source": [
        "## Test data(Use private dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MgWLnU_whVZ"
      },
      "source": [
        "#### Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "3Nf6FJZuvGIY"
      },
      "outputs": [],
      "source": [
        "# === Read data ===\n",
        "with open(\"private_dataset.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# === Split/Chunk ===\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=120,\n",
        "    length_function=len,\n",
        "    separators=[\".\", \" \"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "s0qiqozLvGIY"
      },
      "outputs": [],
      "source": [
        "section_counter = Counter()\n",
        "\n",
        "for text in df[\"full_text\"]:\n",
        "    sections = re.findall(r\"(^|\\n)([A-Z][a-zA-Z \\-/]{2,100})\\n\", text)\n",
        "    for _, sec in sections:\n",
        "        section_counter[sec.strip()] += 1\n",
        "\n",
        "# === Section ===\n",
        "for sec, count in section_counter.most_common():\n",
        "    print(f\"{sec}: {count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ByEFRuUHvGIY"
      },
      "outputs": [],
      "source": [
        "# === Data Preprocesssing ===\n",
        "def preprocess_text(text):\n",
        "    return re.sub(r'https?://\\S+', lambda m: f\"[URL]\", text)\n",
        "\n",
        "remove_sections = {\n",
        "    \"References\",\n",
        "    \"Acknowledgments\",\n",
        "    \"Acknowledgements\",\n",
        "    \"Acknowledgment\",\n",
        "    \"Acknowledgement\",\n",
        "    \"Appendix\",\n",
        "}\n",
        "\n",
        "def remove_sections_from_text(text, remove_sections):\n",
        "    pattern = re.compile(r\"\\n{3}([A-Z][a-zA-Z \\-/]{2,100})\\n\")\n",
        "    matches = list(pattern.finditer(text))\n",
        "    remove_set = {s.lower() for s in remove_sections}\n",
        "    spans_to_remove = []\n",
        "    removed_sections = []\n",
        "\n",
        "    for i, match in enumerate(matches):\n",
        "        section_title = match.group(1).strip()\n",
        "        if section_title.lower() in remove_set:\n",
        "            start = match.start()\n",
        "            end = matches[i + 1].start() if i + 1 < len(matches) else len(text)\n",
        "            removed_text = text[start:end]\n",
        "            removed_sections.append((section_title, removed_text))\n",
        "            spans_to_remove.append((start, end))\n",
        "\n",
        "    for start, end in reversed(spans_to_remove):\n",
        "        text = text[:start] + text[end:]\n",
        "\n",
        "    return text, removed_sections\n",
        "\n",
        "def clean_chunk(chunk):\n",
        "    cleaned = chunk.lstrip(\". \").strip()\n",
        "\n",
        "\n",
        "    if cleaned:\n",
        "        last_sentence = re.split(r'[.?!]\\s+', cleaned)[-1]\n",
        "        if not re.search(r'[.?!]$', last_sentence):\n",
        "            cleaned += \".\"\n",
        "\n",
        "    return cleaned\n",
        "\n",
        "def chunk_with_section_titles(full_text, title=\"\"):\n",
        "    full_text = preprocess_text(full_text)\n",
        "    if not full_text.strip():\n",
        "        return []\n",
        "\n",
        "    sections = full_text.split(\"\\n\\n\\n\")\n",
        "    remove_set = {s.lower() for s in remove_sections}\n",
        "    all_chunks = []\n",
        "\n",
        "    for sec_index, section in enumerate(sections):\n",
        "        lines = section.strip().split(\"\\n\", 1)\n",
        "\n",
        "        # 嘗試拆出 section 標題與內容\n",
        "        if len(lines) == 2:\n",
        "            section_title = lines[0].strip()\n",
        "            content = lines[1].strip()\n",
        "\n",
        "            # 如果是要移除的 section，跳過\n",
        "            if section_title.lower() in remove_set:\n",
        "                print(\"=\" * 60)\n",
        "                print(f\"🔴 刪除了 section: {section_title}\")\n",
        "                print(section.strip())\n",
        "                print(\"=\" * 60)\n",
        "                continue\n",
        "        else:\n",
        "            content = lines[0].strip()\n",
        "\n",
        "        # 對內容進行 chunk 切分（不包含 section 標題）\n",
        "        chunks = text_splitter.split_text(content)\n",
        "        cleaned_chunks = [clean_chunk(c) for c in chunks if clean_chunk(c)]\n",
        "\n",
        "        all_chunks.extend(cleaned_chunks)\n",
        "\n",
        "    return all_chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPsTq6r1w27o"
      },
      "source": [
        "#### Add Data in Vector Store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "eykiU_dTvGIZ"
      },
      "outputs": [],
      "source": [
        "documents_section = []\n",
        "\n",
        "for i, row in df.iterrows():\n",
        "    title = row.get(\"title\", \"\")\n",
        "    full_text = row.get(\"full_text\", \"\")\n",
        "\n",
        "    # 第二種 主題/段落 chunking\n",
        "    for j, chunk in enumerate(chunk_with_section_titles(full_text, title)):\n",
        "        documents_section.append(Document(\n",
        "            page_content=chunk,\n",
        "            metadata={\n",
        "                \"chunk_id\": f\"{i}_{j}\",\n",
        "                \"title\": title\n",
        "            }\n",
        "        ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4b35_PM1vGIZ",
        "outputId": "c24d2a38-6ee9-46e9-8d84-cff627d00d95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ 已成功將所有 chunk 加入 ChromaDB！\n"
          ]
        }
      ],
      "source": [
        "# === Chromadb ===\n",
        "chroma_client = chromadb.PersistentClient(path=\"/content/chroma_dbtest2\")\n",
        "\n",
        "# === Embedding ===\n",
        "embedding_function = SentenceTransformerEmbeddingFunction(\n",
        "    model_name=\"BAAI/bge-base-en-v1.5\"\n",
        ")\n",
        "\n",
        "try:\n",
        "    chroma_client.delete_collection(\"my_collection\")\n",
        "except:\n",
        "    pass\n",
        "\n",
        "collection = chroma_client.get_or_create_collection(\n",
        "    name=\"my_collection\",\n",
        "    embedding_function=embedding_function\n",
        ")\n",
        "\n",
        "# === Add data in Chromadb ===\n",
        "def add_documents_to_chroma(docs, prefix=\"doc\"):\n",
        "    batch_size = 100\n",
        "    for i in range(0, len(docs), batch_size):\n",
        "        batch = docs[i:i + batch_size]\n",
        "        texts = [doc.page_content for doc in batch]\n",
        "        metadatas = [doc.metadata for doc in batch]\n",
        "        ids = [f\"{prefix}_{i + j}\" for j in range(len(batch))]\n",
        "\n",
        "        for m in metadatas:\n",
        "            for k, v in m.items():\n",
        "                if isinstance(v, (list, dict)):\n",
        "                    m[k] = json.dumps(v)\n",
        "\n",
        "        collection.add(\n",
        "            documents=texts,\n",
        "            metadatas=metadatas,\n",
        "            ids=ids\n",
        "        )\n",
        "\n",
        "\n",
        "add_documents_to_chroma(documents_section)\n",
        "\n",
        "print(\"✅ 已成功將所有 chunk 加入 ChromaDB！\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "VWd35Q2cvGIa",
        "outputId": "d76d47be-ef46-41f5-d524-3fb71afbba95"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/chroma_dbtest2.zip'"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "shutil.make_archive(\"/content/chroma_dbtest2\", 'zip', \"/content/chroma_dbtest2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpVDTxeqvGIa"
      },
      "outputs": [],
      "source": [
        "# === Use Llama for Question Generation, Temp Answer Generation ===\n",
        "from transformers import AutoModelForCausalLM\n",
        "from huggingface_hub import login\n",
        "\n",
        "login(\"hf_xxxxxx\")  # 換成你的 API Token\n",
        "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"  # 或改成你訓練過的模型\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model_judge = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", torch_dtype=\"auto\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8K-B9Dwwzz2",
        "outputId": "8d98679f-b69a-4376-a3dd-243e290c426a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_judge,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=128,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    eos_token_id=[tokenizer.eos_token_id,tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")],\n",
        "    do_sample=False\n",
        ")\n",
        "llm_answer = HuggingFacePipeline(pipeline=pipe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEp7HxklvGIa"
      },
      "outputs": [],
      "source": [
        "# === Read data from Chromadb ===\n",
        "embedding = HuggingFaceEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\")\n",
        "vector_store = Chroma(\n",
        "    collection_name=\"my_collection\",\n",
        "    embedding_function=embedding,\n",
        "    persist_directory=\"/content/chroma_dbtest2\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pr4_q63LvGIb"
      },
      "outputs": [],
      "source": [
        "# ===== My answer LLM ======\n",
        "llm = ChatGroq(\n",
        "    model=\"llama-3.1-8b-instant\",\n",
        "    api_key=\"gsk_aaaaaaa\",\n",
        "    temperature=0.0,\n",
        "    max_tokens=128,\n",
        "    model_kwargs={\n",
        "        \"frequency_penalty\": 0.8,\n",
        "        \"presence_penalty\": 0.4,\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWcVPy5lvjKy"
      },
      "outputs": [],
      "source": [
        "# ====== Prompt =====\n",
        "def build_single_input_prompt(title: str) -> ChatPromptTemplate:\n",
        "    \"\"\"\n",
        "    title 為主題，context 為檢索出來的相關段落，question 為 QA 問題。\n",
        "    \"\"\"\n",
        "    system_text = (\n",
        "        \"You are a helpful assistant. Use the retrieved context to answer the question accurately.\\n\\n\"\n",
        "        \"Answer a question about the following topic:\\n\"\n",
        "        f\"=== Topic ===\\n{title.strip()}\\n\\n\"\n",
        "        \"Use the topic to identify the most relevant information in the retrieved content.\"\n",
        "    )\n",
        "\n",
        "    user_text = (\n",
        "        \"Instructions:\\n\"\n",
        "        \"- Carefully read the context and question.\\n\"\n",
        "        \"- Only use the retrieved context below to answer.\\n\"\n",
        "        \"- Do not add any outside information.\\n\"\n",
        "\n",
        "        \"=== Retrieved Context ===\\n{{context}}\\n\\n\"\n",
        "        \"Question: {{question}}\\n\"\n",
        "        \"Respond in this format:\\n\"\n",
        "        \"Answer: <brief answer>\\n\"\n",
        "        \"Think step by step before answering.\\n\\n\"\n",
        "\n",
        "    )\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages(\n",
        "        [(\"system\", system_text), (\"user\", user_text)],\n",
        "        template_format=\"jinja2\"\n",
        "    )\n",
        "\n",
        "    assert set(prompt.input_variables) == {\"context\", \"question\"}\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3iuphJovjKy"
      },
      "outputs": [],
      "source": [
        "def generate_queries_from_llama(question: str) -> list[str]:\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": (\n",
        "            \"You are a helpful assistant that expands a given research question into 1 additional diverse search queries.\\n\"\n",
        "            \"These queries should cover different perspectives, subtopics, or alternative phrasings of the original question.\\n\"\n",
        "            \"Return exactly 1 queries, one per line, without any explanations or numbering.\"\n",
        "        )},\n",
        "        {\"role\": \"user\", \"content\": f\"Original Question: {question}\\nGenerate 1 related but diverse search queries:\"}\n",
        "    ]\n",
        "\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        messages, return_tensors=\"pt\", add_generation_prompt=True\n",
        "    ).to(model_judge.device)\n",
        "\n",
        "    terminators = [\n",
        "        tokenizer.eos_token_id,\n",
        "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "    ]\n",
        "\n",
        "    output_ids = model_judge.generate(\n",
        "        input_ids=input_ids,\n",
        "        max_new_tokens=64,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        eos_token_id=terminators,\n",
        "        do_sample=False,\n",
        "    )\n",
        "\n",
        "    output_text = tokenizer.decode(output_ids[0][input_ids.shape[-1]:], skip_special_tokens=True)\n",
        "\n",
        "    expanded_queries = [question.strip()]\n",
        "    expanded_queries += [q.strip(\"-•0123456789. \") for q in output_text.splitlines() if q.strip()]\n",
        "    return expanded_queries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvnheRdvvjKy"
      },
      "outputs": [],
      "source": [
        "# === RRF ===\n",
        "def reciprocal_rank_fusion(results: list[list[Document]], k=60):\n",
        "    fused_scores = {}\n",
        "    for docs in results:\n",
        "        for rank, doc in enumerate(docs):\n",
        "            key = (doc.page_content, json.dumps(doc.metadata, sort_keys=True))  # 使用元組作為唯一識別\n",
        "            fused_scores[key] = fused_scores.get(key, 0) + 1 / (rank + k)\n",
        "\n",
        "    reranked = sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    return [\n",
        "        Document(page_content=content, metadata=json.loads(meta))\n",
        "        for (content, meta), _ in reranked\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9I66Gs6vjKy"
      },
      "outputs": [],
      "source": [
        "def retrieve_with_title_fallback(vector_store, title, question, top_k, verbose=False):\n",
        "\n",
        "    # 先列出所有可用 title\n",
        "    all_titles = list({\n",
        "        m.get(\"title\") for m in vector_store._collection.get(include=[\"metadatas\"])[\"metadatas\"]\n",
        "        if \"title\" in m\n",
        "    })\n",
        "\n",
        "    # 嘗試找到最相近的 title（避免空格、dash、引號不一致）\n",
        "    matched = difflib.get_close_matches(title, all_titles, n=1)\n",
        "    used_title = matched[0] if matched else title\n",
        "    # Step 1: title 相符的 documents\n",
        "    results = vector_store._collection.query(\n",
        "        query_texts=[question],\n",
        "        n_results=top_k,\n",
        "        where={\"title\": used_title}\n",
        "    )\n",
        "\n",
        "\n",
        "    docs = [\n",
        "        Document(page_content=text, metadata=meta)\n",
        "        for text, meta in zip(results[\"documents\"][0], results[\"metadatas\"][0])\n",
        "    ]\n",
        "\n",
        "    # Step 2: 若不足 top_k，補其他主題\n",
        "    if len(docs) < top_k:\n",
        "        needed = top_k - len(docs)\n",
        "        fallback_results = vector_store._collection.query(\n",
        "            query_texts=[question],\n",
        "            n_results=needed,\n",
        "            where={\"title\": {\"$ne\": used_title}}\n",
        "        )\n",
        "        fallback_docs = [\n",
        "            Document(page_content=text, metadata=meta)\n",
        "            for text, meta in zip(fallback_results[\"documents\"][0], fallback_results[\"metadatas\"][0])\n",
        "        ]\n",
        "        docs.extend(fallback_docs)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"\\nTop {top_k} Retrieved Chunks for title: '{title}' (matched: '{used_title}')\")\n",
        "        for idx, doc in enumerate(docs):\n",
        "            print(f\"\\n--- Retrieved Chunk #{idx+1} ---\")\n",
        "            print(f\"Title: {doc.metadata.get('title', '[No title]')}\")\n",
        "            print(f\"Chunk ID: {doc.metadata.get('chunk_id')}\")\n",
        "            print(f\"Content:\\n{doc.page_content[:500]}...\")\n",
        "\n",
        "    return docs[:top_k]\n",
        "\n",
        "class StaticRetriever(BaseRetriever):\n",
        "    docs: List[Document]\n",
        "    def _get_relevant_documents(self, query: str) -> List[Document]:\n",
        "        return self.docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-LgIzVZvjKz"
      },
      "outputs": [],
      "source": [
        "reranker = CrossEncoder(\"BAAI/bge-reranker-large\", device=\"cuda\")\n",
        "def rerank_with_answer_based_bge(answer: str, docs: list[Document], top_k: int = 5, threshold: float = 0.6, verbose=False):\n",
        "    pairs = [[answer, doc.page_content] for doc in docs]\n",
        "    scores = reranker.predict(pairs)\n",
        "    reranked = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    if verbose:\n",
        "        print(\"\\n📌 Rerank by Answer Similarity:\")\n",
        "        for i, (doc, score) in enumerate(reranked[:top_k]):\n",
        "            print(f\"# {i+1} | Score: {score:.4f} | Preview: {doc.page_content[:200]}...\")\n",
        "\n",
        "    return [doc for doc, score in reranked[:top_k] if score >= threshold] or [doc for doc, _ in reranked[:top_k]]\n",
        "\n",
        "\n",
        "# === Retrieval based on RAG Fusion ===\n",
        "def retrieve_rag_fusion(vector_store, title: str, question: str, per_query_k: int = 10, final_k: int = 20):\n",
        "    queries = generate_queries_from_llama(question)\n",
        "\n",
        "    all_results = []\n",
        "\n",
        "    for idx, query in enumerate(queries):\n",
        "        docs = retrieve_with_title_fallback(vector_store, title, query, top_k=per_query_k)\n",
        "        all_results.append(docs)\n",
        "        print(f\"  - Query {idx+1}: {query} => {len(docs)} docs\")  # debug info\n",
        "\n",
        "    fused_docs = reciprocal_rank_fusion(all_results)\n",
        "\n",
        "    print(\"\\n🔁 RRF Top Documents:\")\n",
        "    for i, doc in enumerate(fused_docs[:final_k]):\n",
        "        print(f\"  🔁 RRF #{i+1} | Title: {doc.metadata.get('title')} | Preview: {doc.page_content[:200]}...\")\n",
        "\n",
        "    return fused_docs[:final_k]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pjaW-ZJvjKz"
      },
      "outputs": [],
      "source": [
        "PER_QUERY_K = 10\n",
        "FINAL_K_RRF = 15\n",
        "FINAL_K_RERANK = 5\n",
        "answers = []\n",
        "results = []\n",
        "\n",
        "\n",
        "checkpoint_path = \"ragquerybgeanswerstd0.610_15_5test8b.json\"\n",
        "\n",
        "# === checkpoint）===\n",
        "if os.path.exists(checkpoint_path):\n",
        "    with open(checkpoint_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        results = json.load(f)\n",
        "    done_indices = {r[\"title\"] for r in results}\n",
        "else:\n",
        "    results = []\n",
        "    done_indices = set()\n",
        "\n",
        "# === 主迴圈：遍歷驗證集 ===\n",
        "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
        "    title = row[\"title\"].strip()\n",
        "    if title in done_indices:\n",
        "        continue\n",
        "\n",
        "    full_text = row[\"full_text\"].strip()\n",
        "    question = row[\"question\"].strip()\n",
        "    prompt = build_single_input_prompt(title)\n",
        "\n",
        "    docs_before_rerank = retrieve_rag_fusion(\n",
        "        vector_store, title, question,\n",
        "        per_query_k=PER_QUERY_K, final_k=FINAL_K_RRF\n",
        "    )\n",
        "\n",
        "    # Step 1: Predict Temp Answer\n",
        "    temp_retriever = StaticRetriever(docs=docs_before_rerank)\n",
        "    temp_chain = RetrievalQA.from_chain_type(\n",
        "        llm=llm_answer,\n",
        "        retriever=temp_retriever,\n",
        "        chain_type=\"stuff\",\n",
        "        chain_type_kwargs={\"prompt\": prompt},\n",
        "        return_source_documents=True\n",
        "    )\n",
        "    temp_result = temp_chain.invoke({\"query\": question})\n",
        "    output_text = temp_result[\"result\"]\n",
        "\n",
        "    predicted_answer = \"\"\n",
        "    for line in output_text.splitlines():\n",
        "        if line.lower().startswith(\"answer:\"):\n",
        "            predicted_answer = line[len(\"answer:\"):].strip()\n",
        "\n",
        "    # Step 2: Rerank by Temp Answer\n",
        "    fused_docs = rerank_with_answer_based_bge(predicted_answer, docs_before_rerank, top_k=FINAL_K_RERANK)\n",
        "    final_retriever = StaticRetriever(docs=fused_docs)\n",
        "\n",
        "    # Step 3: Answer\n",
        "    rag_chain = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        retriever=final_retriever,\n",
        "        chain_type=\"stuff\",\n",
        "        chain_type_kwargs={\"prompt\": prompt},\n",
        "        return_source_documents=True\n",
        "    )\n",
        "    result = rag_chain.invoke({\"query\": question})\n",
        "\n",
        "    answer_text = \"\"\n",
        "    for line in result[\"result\"].splitlines():\n",
        "        if line.lower().startswith(\"answer:\"):\n",
        "            answer_text = line[len(\"answer:\"):].strip()\n",
        "\n",
        "    retrieved_chunks = [doc.page_content for doc in fused_docs]\n",
        "\n",
        "    item = {\n",
        "        \"title\": title,\n",
        "        \"answer\": answer_text,\n",
        "        \"evidence\": retrieved_chunks\n",
        "\n",
        "    }\n",
        "    results.append(item)\n",
        "\n",
        "    with open(checkpoint_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(results, f, ensure_ascii=False, indent=2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0pf-TWmxFQf"
      },
      "source": [
        "## Train data(Public Dataset)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "JaQHjBysxFQf"
      },
      "outputs": [],
      "source": [
        "# === Read Data ===\n",
        "with open(\"public_dataset.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# === Chunk/Split ===\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=120,\n",
        "    length_function=len,\n",
        "    separators=[\".\", \" \"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "wOss6ijKpVWA"
      },
      "outputs": [],
      "source": [
        "# === Section ===\n",
        "section_counter = Counter()\n",
        "\n",
        "for text in df[\"full_text\"]:\n",
        "    sections = re.findall(r\"(^|\\n)([A-Z][a-zA-Z \\-/]{2,100})\\n\", text)\n",
        "    for _, sec in sections:\n",
        "        section_counter[sec.strip()] += 1\n",
        "\n",
        "for sec, count in section_counter.most_common():\n",
        "    print(f\"{sec}: {count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7LpwSBKxFQg"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    return re.sub(r'https?://\\S+', lambda m: f\"[URL]\", text)\n",
        "remove_sections = {\n",
        "    \"References\", \"Acknowledgments\", \"Appendix\",\"Acknowledgements\"\n",
        "}\n",
        "\n",
        "def remove_sections_from_text(text, remove_sections):\n",
        "    pattern = re.compile(r\"\\n{3}([A-Z][a-zA-Z \\-/]{2,100})\\n\")\n",
        "    matches = list(pattern.finditer(text))\n",
        "    remove_set = {s.lower() for s in remove_sections}\n",
        "    spans_to_remove = []\n",
        "    removed_sections = []\n",
        "\n",
        "    for i, match in enumerate(matches):\n",
        "        section_title = match.group(1).strip()\n",
        "        if section_title.lower() in remove_set:\n",
        "            start = match.start()\n",
        "            end = matches[i + 1].start() if i + 1 < len(matches) else len(text)\n",
        "            removed_text = text[start:end]\n",
        "            removed_sections.append((section_title, removed_text))\n",
        "            spans_to_remove.append((start, end))\n",
        "\n",
        "    for start, end in reversed(spans_to_remove):\n",
        "        text = text[:start] + text[end:]\n",
        "\n",
        "    return text, removed_sections\n",
        "def clean_chunk(chunk):\n",
        "    cleaned = chunk.lstrip(\". \").strip()\n",
        "\n",
        "    if cleaned:\n",
        "        last_sentence = re.split(r'[.?!]\\s+', cleaned)[-1]\n",
        "        if not re.search(r'[.?!]$', last_sentence):\n",
        "            cleaned += \".\"\n",
        "\n",
        "    return cleaned\n",
        "\n",
        "def chunk_with_section_titles(full_text, title=\"\"):\n",
        "    full_text = preprocess_text(full_text)\n",
        "    if not full_text.strip():\n",
        "        return []\n",
        "\n",
        "    sections = full_text.split(\"\\n\\n\\n\")\n",
        "    remove_set = {s.lower() for s in remove_sections}\n",
        "    all_chunks = []\n",
        "\n",
        "    for sec_index, section in enumerate(sections):\n",
        "        lines = section.strip().split(\"\\n\", 1)\n",
        "\n",
        "        if len(lines) == 2:\n",
        "            section_title = lines[0].strip()\n",
        "            content = lines[1].strip()\n",
        "\n",
        "\n",
        "            if section_title.lower() in remove_set:\n",
        "                print(\"=\" * 60)\n",
        "                print(f\"🔴 刪除了 section: {section_title}\")\n",
        "                print(section.strip())\n",
        "                print(\"=\" * 60)\n",
        "                continue\n",
        "        else:\n",
        "            content = lines[0].strip()\n",
        "\n",
        "        chunks = text_splitter.split_text(content)\n",
        "        cleaned_chunks = [clean_chunk(c) for c in chunks if clean_chunk(c)]\n",
        "\n",
        "        all_chunks.extend(cleaned_chunks)\n",
        "\n",
        "    return all_chunks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SsYmBtzxFQg"
      },
      "source": [
        "#### Add Data in Vector Store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXAvmD2ZxFQg"
      },
      "outputs": [],
      "source": [
        "documents_section = []\n",
        "\n",
        "for i, row in df.iterrows():\n",
        "    title = row.get(\"title\", \"\")\n",
        "    full_text = row.get(\"full_text\", \"\")\n",
        "\n",
        "    for j, chunk in enumerate(chunk_with_section_titles(full_text, title)):\n",
        "        documents_section.append(Document(\n",
        "            page_content=chunk,\n",
        "            metadata={\n",
        "                \"chunk_id\": f\"{i}_{j}\",\n",
        "                \"title\": title\n",
        "            }\n",
        "        ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4PaRqX6xFQg"
      },
      "outputs": [],
      "source": [
        "# === ChromaDB===\n",
        "chroma_client = chromadb.PersistentClient(path=\"/content/chroma_db10\")\n",
        "\n",
        "# === embedding function===\n",
        "embedding_function = SentenceTransformerEmbeddingFunction(\n",
        "    model_name=\"BAAI/bge-base-en-v1.5\"\n",
        ")\n",
        "\n",
        "try:\n",
        "    chroma_client.delete_collection(\"my_collection\")\n",
        "except:\n",
        "    pass\n",
        "\n",
        "collection = chroma_client.get_or_create_collection(\n",
        "    name=\"my_collection\",\n",
        "    embedding_function=embedding_function\n",
        ")\n",
        "\n",
        "# === Add data in ChromaDB ===\n",
        "def add_documents_to_chroma(docs, prefix=\"doc\"):\n",
        "    batch_size = 100\n",
        "    for i in range(0, len(docs), batch_size):\n",
        "        batch = docs[i:i + batch_size]\n",
        "        texts = [doc.page_content for doc in batch]\n",
        "        metadatas = [doc.metadata for doc in batch]\n",
        "        ids = [f\"{prefix}_{i + j}\" for j in range(len(batch))]\n",
        "\n",
        "        for m in metadatas:\n",
        "            for k, v in m.items():\n",
        "                if isinstance(v, (list, dict)):\n",
        "                    m[k] = json.dumps(v)\n",
        "\n",
        "        collection.add(\n",
        "            documents=texts,\n",
        "            metadatas=metadatas,\n",
        "            ids=ids\n",
        "        )\n",
        "\n",
        "\n",
        "add_documents_to_chroma(documents_section)\n",
        "\n",
        "\n",
        "print(\"✅ 已成功將所有 chunk 加入 ChromaDB！\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "FSkvYDgYxFQh",
        "outputId": "e73bc98a-8fc3-4237-84aa-0fe3941430e0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/chroma_db10.zip'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "shutil.make_archive(\"/content/chroma_db10\", 'zip', \"/content/chroma_db10\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3yTW07uxFQh"
      },
      "outputs": [],
      "source": [
        "# === Step 2: 建立 LLM（使用 huggingface pipeline 包裝）===\n",
        "from transformers import AutoModelForCausalLM\n",
        "from huggingface_hub import login\n",
        "\n",
        "login(\"hf_xxxxx\")  # 換成你的 API Token\n",
        "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"  # 或改成你訓練過的模型\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model_judge = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", torch_dtype=\"auto\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zmWjCc2xFQi",
        "outputId": "087a2cdd-c487-403f-def5-b7b703e43221"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_judge,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=100,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    eos_token_id=[tokenizer.eos_token_id,tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")],\n",
        "    do_sample=False\n",
        ")\n",
        "llm = HuggingFacePipeline(pipeline=pipe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73TmxpQhxFQi"
      },
      "outputs": [],
      "source": [
        "# === Read ChromaDB ===\n",
        "embedding = HuggingFaceEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\")\n",
        "vector_store = Chroma(\n",
        "    collection_name=\"my_collection\",\n",
        "    embedding_function=embedding,\n",
        "    persist_directory=\"/content/chroma_db10\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYxCU5d0v5ke"
      },
      "source": [
        "#### Retrival Method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4x-V94-ZxFQi"
      },
      "outputs": [],
      "source": [
        "with open(\"public_dataset.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "df = pd.DataFrame(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XvwfRIo_xFQj"
      },
      "outputs": [],
      "source": [
        "##================Prompt======================\n",
        "def build_single_input_prompt(title: str) -> ChatPromptTemplate:\n",
        "    \"\"\"\n",
        "    title 為主題，context 為檢索出來的相關段落，question 為 QA 問題。\n",
        "    \"\"\"\n",
        "    system_text = (\n",
        "        \"You are a helpful assistant. Use the retrieved context to answer the question accurately.\\n\\n\"\n",
        "        \"Answer a question about the following topic:\\n\"\n",
        "        f\"=== Topic ===\\n{title.strip()}\\n\\n\"\n",
        "        \"Use the topic to identify the most relevant information in the retrieved content.\"\n",
        "    )\n",
        "\n",
        "    user_text = (\n",
        "        \"Instructions:\\n\"\n",
        "        \"- Carefully read the context and question.\\n\"\n",
        "        \"- Only use the retrieved context below to answer.\\n\"\n",
        "        \"- Do not add any outside information.\\n\"\n",
        "\n",
        "        \"=== Retrieved Context ===\\n{{context}}\\n\\n\"\n",
        "        \"Question: {{question}}\\n\"\n",
        "        \"Respond in this format:\\n\"\n",
        "        \"Answer: <brief answer>\\n\"\n",
        "        \"Think step by step before answering.\\n\\n\"\n",
        "\n",
        "    )\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages(\n",
        "        [(\"system\", system_text), (\"user\", user_text)],\n",
        "        template_format=\"jinja2\"\n",
        "    )\n",
        "\n",
        "    assert set(prompt.input_variables) == {\"context\", \"question\"}\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0nlXBYGxFQj"
      },
      "outputs": [],
      "source": [
        "##===============Retrieval==============\n",
        "def retrieve_with_title_fallback(vector_store, title, question, top_k, verbose=False):\n",
        "\n",
        "    all_titles = list({\n",
        "        m.get(\"title\") for m in vector_store._collection.get(include=[\"metadatas\"])[\"metadatas\"]\n",
        "        if \"title\" in m\n",
        "    })\n",
        "\n",
        "    # 嘗試找到最相近的 title（避免空格、dash、引號不一致）\n",
        "    matched = difflib.get_close_matches(title, all_titles, n=1)\n",
        "    used_title = matched[0] if matched else title\n",
        "    # Step 1: title 相符的 documents\n",
        "    results = vector_store._collection.query(\n",
        "        query_texts=[question],\n",
        "        n_results=top_k,\n",
        "        where={\"title\": used_title}\n",
        "    )\n",
        "\n",
        "    docs = [\n",
        "        Document(page_content=text, metadata=meta)\n",
        "        for text, meta in zip(results[\"documents\"][0], results[\"metadatas\"][0])\n",
        "    ]\n",
        "\n",
        "    # Step 2: 若不足 top_k，補其他主題\n",
        "    if len(docs) < top_k:\n",
        "        needed = top_k - len(docs)\n",
        "        fallback_results = vector_store._collection.query(\n",
        "            query_texts=[question],\n",
        "            n_results=needed,\n",
        "            where={\"title\": {\"$ne\": used_title}}\n",
        "        )\n",
        "        fallback_docs = [\n",
        "            Document(page_content=text, metadata=meta)\n",
        "            for text, meta in zip(fallback_results[\"documents\"][0], fallback_results[\"metadatas\"][0])\n",
        "        ]\n",
        "        docs.extend(fallback_docs)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"\\nTop {top_k} Retrieved Chunks for title: '{title}' (matched: '{used_title}')\")\n",
        "        for idx, doc in enumerate(docs):\n",
        "            print(f\"\\n--- Retrieved Chunk #{idx+1} ---\")\n",
        "            print(f\"Title: {doc.metadata.get('title', '[No title]')}\")\n",
        "            print(f\"Chunk ID: {doc.metadata.get('chunk_id')}\")\n",
        "            print(f\"Content:\\n{doc.page_content[:500]}...\")\n",
        "\n",
        "    return docs[:top_k]\n",
        "\n",
        "class StaticRetriever(BaseRetriever):\n",
        "    docs: List[Document]\n",
        "    def _get_relevant_documents(self, query: str) -> List[Document]:\n",
        "        return self.docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tO9pHpJZr5EZ"
      },
      "source": [
        "##### 1. Step1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VnEvMrcxFQj"
      },
      "source": [
        "###### **1.** Query→Retrieve Top K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vcMy4r1xxFQj"
      },
      "outputs": [],
      "source": [
        "TOP_K = 8\n",
        "answers = []\n",
        "results = []\n",
        "\n",
        "\n",
        "checkpoint_path = \"rag500_8.json\"\n",
        "\n",
        "if os.path.exists(checkpoint_path):\n",
        "    with open(checkpoint_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        results = json.load(f)\n",
        "    done_indices = {r[\"title\"] for r in results}  # 用 title 作為索引（視情況也可加 question）\n",
        "else:\n",
        "    results = []\n",
        "    done_indices = set()\n",
        "\n",
        "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
        "    title = row[\"title\"].strip()\n",
        "    if title in done_indices:\n",
        "        continue  # 跳過已處理過的\n",
        "\n",
        "    full_text = row[\"full_text\"].strip()\n",
        "    question = row[\"question\"].strip()\n",
        "    ground_truth_answer = \" \".join(row[\"answer\"]).strip()\n",
        "    ground_truth_evidence = \" \".join(row[\"evidence\"]).strip()\n",
        "\n",
        "    prompt = build_single_input_prompt(title)\n",
        "\n",
        "    retrieved_docs = retrieve_with_title_fallback(vector_store, title, question, top_k=TOP_K)\n",
        "    retriever = StaticRetriever(docs=retrieved_docs)\n",
        "\n",
        "    rag_chain = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        retriever=retriever,\n",
        "        chain_type=\"stuff\",\n",
        "        chain_type_kwargs={\"prompt\": prompt},\n",
        "        return_source_documents=True\n",
        "    )\n",
        "\n",
        "    result = rag_chain.invoke({\"query\": question})\n",
        "    output_text = result[\"result\"]\n",
        "    retrieved_chunks = [doc.page_content for doc in result[\"source_documents\"]]\n",
        "    evidence_text = \" \".join(retrieved_chunks)\n",
        "\n",
        "\n",
        "    answer_text = \"\"\n",
        "    for line in output_text.splitlines():\n",
        "        if line.lower().startswith(\"answer:\"):\n",
        "            answer_text = line[len(\"answer:\"):].strip()\n",
        "\n",
        "\n",
        "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "    rouge_result = scorer.score(ground_truth_evidence, evidence_text)\n",
        "    rouge_l_score = rouge_result[\"rougeL\"].fmeasure\n",
        "\n",
        "    item = {\n",
        "        \"title\": title,\n",
        "        \"question\": question,\n",
        "        \"predicted_answer\": answer_text,\n",
        "        \"ground_truth_answer\": ground_truth_answer,\n",
        "        \"predicted_evidence\": retrieved_chunks,\n",
        "        \"ground_truth_evidence\": ground_truth_evidence,\n",
        "        \"rougeL\": round(rouge_l_score, 4),\n",
        "    }\n",
        "    results.append(item)\n",
        "\n",
        "    # === ✅ 寫入 checkpoint（每筆一存）===\n",
        "    with open(checkpoint_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"✅ Checkpoint saved for: {title}\")\n",
        "\n",
        "for idx, res in enumerate(results[:3]):\n",
        "    print(f\"--- Sample {idx+1} ---\")\n",
        "    print(f\"Title: {res['title']}\")\n",
        "    print(f\"Question: {res['question']}\")\n",
        "    print(f\"Predicted Answer: {res['predicted_answer']}\")\n",
        "    print(f\"Ground Truth Answer: {res['ground_truth_answer']}\")\n",
        "    print(f\"Predicted Evidence: {res['predicted_evidence']}\")\n",
        "    print(f\"Ground Truth Evidence: {res['ground_truth_evidence']}\")\n",
        "    print(f\"ROUGE-L: {res['rougeL']}\")\n",
        "    print(\"-\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QAYoxkxxFQk",
        "outputId": "1e745bd0-428c-43c1-c903-d4aac98734ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== 📊 驗證集整體評估結果 ===\n",
            "📈 平均 ROUGE-L 分數：0.1749\n",
            "📈 超多證據：0.0000\n",
            "📈 一半證據：0.0000\n",
            "📈 有提到證據：17.0000\n",
            "📈 平均 ROUGE-L 分數：0.1749\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# === 將結果轉成 DataFrame ===\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "rouge_scores = [res[\"rougeL\"] for res in results]\n",
        "\n",
        "avg_rouge = np.mean(rouge_scores)\n",
        "lot_evidence = sum(1 for s in rouge_scores if s >= 0.7)\n",
        "half_evidence = sum(1 for s in rouge_scores if s >= 0.5)\n",
        "enough_evidence = sum(1 for s in rouge_scores if s >= 0.3)\n",
        "\n",
        "print(\"\\n=== 📊 驗證集整體評估結果 ===\")\n",
        "print(f\"📈 平均 ROUGE-L 分數：{avg_rouge:.4f}\")\n",
        "print(f\"📈 超多證據：{lot_evidence:.4f}\")\n",
        "print(f\"📈 一半證據：{half_evidence:.4f}\")\n",
        "print(f\"📈 有提到證據：{enough_evidence:.4f}\")\n",
        "print(f\"📈 平均 ROUGE-L 分數：{avg_rouge:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99_2bvVQr-HM"
      },
      "source": [
        "##### 2. Step2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACMu1TWrxFQl"
      },
      "source": [
        "###### **2.** Query→Retrieve→Rerank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGGut9OOxFQm"
      },
      "outputs": [],
      "source": [
        "bge_tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-reranker-large\")\n",
        "bge_model = AutoModelForSequenceClassification.from_pretrained(\"BAAI/bge-reranker-large\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ll3fk3GfxFQm"
      },
      "outputs": [],
      "source": [
        "bge_model.eval()\n",
        "bge_model.cuda()\n",
        "def rerank_with_bge(question: str, docs: list, top_k: int = 8):\n",
        "    pairs = [(question, doc.page_content) for doc in docs]\n",
        "\n",
        "    encoded = bge_tokenizer.batch_encode_plus(\n",
        "        pairs,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\",\n",
        "        max_length=512\n",
        "    ).to(bge_model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        scores = bge_model(**encoded).logits.squeeze(-1)\n",
        "\n",
        "\n",
        "    sorted_results = sorted(zip(docs, scores.tolist()), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    return [doc for doc, _ in sorted_results[:top_k]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "NcNq5oj5xFQn"
      },
      "outputs": [],
      "source": [
        "TOP_K = 15\n",
        "RERANK_K = 5\n",
        "\n",
        "answers = []\n",
        "results = []\n",
        "\n",
        "checkpoint_path = \"rag500_15_5.json\"\n",
        "\n",
        "\n",
        "if os.path.exists(checkpoint_path):\n",
        "    with open(checkpoint_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        results = json.load(f)\n",
        "    done_indices = {r[\"title\"] for r in results}  # 用 title 作為索引（視情況也可加 question）\n",
        "else:\n",
        "    results = []\n",
        "    done_indices = set()\n",
        "\n",
        "# === 主迴圈：遍歷驗證集 ===\n",
        "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
        "    title = row[\"title\"].strip()\n",
        "    if title in done_indices:\n",
        "        continue\n",
        "\n",
        "    full_text = row[\"full_text\"].strip()\n",
        "    question = row[\"question\"].strip()\n",
        "    ground_truth_answer = \" \".join(row[\"answer\"]).strip()\n",
        "    ground_truth_evidence = \" \".join(row[\"evidence\"]).strip()\n",
        "\n",
        "    prompt = build_single_input_prompt(title)\n",
        "\n",
        "    retrieved_docs = retrieve_with_title_fallback(vector_store, title, question, top_k=TOP_K)\n",
        "    reranked_docs = rerank_with_bge(question, retrieved_docs, top_k=RERANK_K)\n",
        "    retriever = StaticRetriever(docs=reranked_docs)\n",
        "\n",
        "\n",
        "    rag_chain = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        retriever=retriever,\n",
        "        chain_type=\"stuff\",\n",
        "        chain_type_kwargs={\"prompt\": prompt},\n",
        "        return_source_documents=True\n",
        "    )\n",
        "\n",
        "    result = rag_chain.invoke({\"query\": question})\n",
        "    output_text = result[\"result\"]\n",
        "\n",
        "    retrieved_chunks = [doc.page_content for doc in result[\"source_documents\"]]\n",
        "    evidence_text = \" \".join(retrieved_chunks)\n",
        "\n",
        "    answer_text = \"\"\n",
        "    for line in output_text.splitlines():\n",
        "        if line.lower().startswith(\"answer:\"):\n",
        "            answer_text = line[len(\"answer:\"):].strip()\n",
        "\n",
        "\n",
        "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "    rouge_result = scorer.score(ground_truth_evidence, evidence_text)\n",
        "    rouge_l_score = rouge_result[\"rougeL\"].fmeasure\n",
        "\n",
        "\n",
        "    item = {\n",
        "        \"title\": title,\n",
        "        \"question\": question,\n",
        "        \"predicted_answer\": answer_text,\n",
        "        \"ground_truth_answer\": ground_truth_answer,\n",
        "        \"predicted_evidence\": retrieved_chunks,\n",
        "        \"ground_truth_evidence\": ground_truth_evidence,\n",
        "        \"rougeL\": round(rouge_l_score, 4),\n",
        "    }\n",
        "    results.append(item)\n",
        "\n",
        "    # === ✅ 寫入 checkpoint（每筆一存）===\n",
        "    with open(checkpoint_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"✅ Checkpoint saved for: {title}\")\n",
        "\n",
        "# === 印出前三筆結果 ===\n",
        "for idx, res in enumerate(results[:3]):\n",
        "    print(f\"--- Sample {idx+1} ---\")\n",
        "    print(f\"Title: {res['title']}\")\n",
        "    print(f\"Question: {res['question']}\")\n",
        "    print(f\"Predicted Answer: {res['predicted_answer']}\")\n",
        "    print(f\"Ground Truth Answer: {res['ground_truth_answer']}\")\n",
        "    print(f\"Predicted Evidence: {res['predicted_evidence']}\")\n",
        "    print(f\"Ground Truth Evidence: {res['ground_truth_evidence']}\")\n",
        "    print(f\"ROUGE-L: {res['rougeL']}\")\n",
        "    print(\"-\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9--zRjryxFQn",
        "outputId": "82f2e382-ca3d-4d57-d478-d9671b6cdc88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== 📊 驗證集整體評估結果 ===\n",
            "📈 平均 ROUGE-L 分數：0.1880\n",
            "📈 超多證據：0.0000\n",
            "📈 一半證據：1.0000\n",
            "📈 有提到證據：20.0000\n",
            "📈 平均 ROUGE-L 分數：0.1880\n"
          ]
        }
      ],
      "source": [
        "#### k=15/5\n",
        "import pandas as pd\n",
        "\n",
        "# === 將結果轉成 DataFrame ===\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "\n",
        "rouge_scores = [res[\"rougeL\"] for res in results]\n",
        "\n",
        "avg_rouge = np.mean(rouge_scores)\n",
        "lot_evidence = sum(1 for s in rouge_scores if s >= 0.7)\n",
        "half_evidence = sum(1 for s in rouge_scores if s >= 0.5)\n",
        "enough_evidence = sum(1 for s in rouge_scores if s >= 0.3)\n",
        "\n",
        "print(\"\\n=== 📊 驗證集整體評估結果 ===\")\n",
        "print(f\"📈 平均 ROUGE-L 分數：{avg_rouge:.4f}\")\n",
        "print(f\"📈 超多證據：{lot_evidence:.4f}\")\n",
        "print(f\"📈 一半證據：{half_evidence:.4f}\")\n",
        "print(f\"📈 有提到證據：{enough_evidence:.4f}\")\n",
        "print(f\"📈 平均 ROUGE-L 分數：{avg_rouge:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaDmtiWpsBp4"
      },
      "source": [
        "##### 3. Step 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "syG5083oz0pH"
      },
      "outputs": [],
      "source": [
        "reranker = CrossEncoder(\"BAAI/bge-reranker-large\", device=\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5cjAYF9xFQu"
      },
      "outputs": [],
      "source": [
        "# === RRF ===\n",
        "def reciprocal_rank_fusion(results: list[list[Document]], k=60):\n",
        "    fused_scores = {}\n",
        "    for docs in results:\n",
        "        for rank, doc in enumerate(docs):\n",
        "            key = (doc.page_content, json.dumps(doc.metadata, sort_keys=True))  # 使用元組作為唯一識別\n",
        "            fused_scores[key] = fused_scores.get(key, 0) + 1 / (rank + k)\n",
        "\n",
        "    reranked = sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    return [\n",
        "        Document(page_content=content, metadata=json.loads(meta))\n",
        "        for (content, meta), _ in reranked\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qe5Av_0HxFQu"
      },
      "outputs": [],
      "source": [
        "def rerank_with_bge(query: str, docs: list[Document], top_k: int = 8):\n",
        "    pairs = [[query, doc.page_content] for doc in docs]\n",
        "    scores = reranker.predict(pairs)\n",
        "    reranked = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    return [doc for doc, _ in reranked[:top_k]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nw2dtRscxFQu"
      },
      "source": [
        "###### **3.** Query Augmentation → RRF → Rerank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jh6DGJ4pxFQu"
      },
      "outputs": [],
      "source": [
        "def generate_queries_from_llama(question: str) -> list[str]:\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": (\n",
        "            \"You are a helpful assistant that expands a given research question into 2 additional diverse search queries.\\n\"\n",
        "            \"These queries should cover different perspectives, subtopics, or alternative phrasings of the original question.\\n\"\n",
        "            \"Return exactly 2 queries, one per line, without any explanations or numbering.\"\n",
        "        )},\n",
        "        {\"role\": \"user\", \"content\": f\"Original Question: {question}\\nGenerate 2 related but diverse search queries:\"}\n",
        "    ]\n",
        "\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        messages, return_tensors=\"pt\", add_generation_prompt=True\n",
        "    ).to(model_judge.device)\n",
        "\n",
        "    terminators = [\n",
        "        tokenizer.eos_token_id,\n",
        "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "    ]\n",
        "\n",
        "    output_ids = model_judge.generate(\n",
        "        input_ids=input_ids,\n",
        "        max_new_tokens=64,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        eos_token_id=terminators,\n",
        "        do_sample=False,\n",
        "    )\n",
        "\n",
        "    output_text = tokenizer.decode(output_ids[0][input_ids.shape[-1]:], skip_special_tokens=True)\n",
        "\n",
        "    # 回傳：原始 + 延伸三個 query\n",
        "    expanded_queries = [question.strip()]  # 原始查詢第一筆\n",
        "    expanded_queries += [q.strip(\"-•0123456789. \") for q in output_text.splitlines() if q.strip()]\n",
        "    return expanded_queries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uivwUDiYuUsN"
      },
      "outputs": [],
      "source": [
        "# === Retrieval based on RAG Fusion ===\n",
        "def retrieve_rag_fusion(vector_store, title: str, question: str, per_query_k: int = 10, final_k: int = 20):\n",
        "    queries = generate_queries_from_llama(question)\n",
        "    all_results = []\n",
        "\n",
        "    for idx, query in enumerate(queries):\n",
        "        docs = retrieve_with_title_fallback(vector_store, title, query, top_k=per_query_k)\n",
        "        all_results.append(docs)\n",
        "        #print(f\"  - Query {idx+1}: {query} => {len(docs)} docs\")  # debug info\n",
        "\n",
        "    fused_docs = reciprocal_rank_fusion(all_results)\n",
        "\n",
        "    print(\"\\n🔁 RRF Top Documents:\")\n",
        "    for i, doc in enumerate(fused_docs[:final_k]):\n",
        "        print(f\"  🔁 RRF #{i+1} | Title: {doc.metadata.get('title')} | Preview: {doc.page_content[:200]}...\")\n",
        "\n",
        "    return fused_docs[:final_k]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "1o4MUtLdlqDn"
      },
      "outputs": [],
      "source": [
        "PER_QUERY_K = 10\n",
        "FINAL_K_RRF = 15\n",
        "FINAL_K_RERANK = 5\n",
        "answers = []\n",
        "results = []\n",
        "\n",
        "\n",
        "checkpoint_path = \"ragfusionbge10_15_5all1.json\"\n",
        "\n",
        "# === 載入已儲存的 checkpoint（若存在）===\n",
        "if os.path.exists(checkpoint_path):\n",
        "    with open(checkpoint_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        results = json.load(f)\n",
        "    done_indices = {r[\"title\"] for r in results}  # 用 title 作為索引（視情況也可加 question）\n",
        "else:\n",
        "    results = []\n",
        "    done_indices = set()\n",
        "\n",
        "\n",
        "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
        "    title = row[\"title\"].strip()\n",
        "    if title in done_indices:\n",
        "        continue  # 跳過已處理過的\n",
        "\n",
        "    full_text = row[\"full_text\"].strip()\n",
        "    question = row[\"question\"].strip()\n",
        "    ground_truth_answer = \" \".join(row[\"answer\"]).strip()\n",
        "    ground_truth_evidence = \" \".join(row[\"evidence\"]).strip()\n",
        "\n",
        "    prompt = build_single_input_prompt(title)\n",
        "\n",
        "    docs_before_rerank = retrieve_rag_fusion(vector_store, title, question, per_query_k=PER_QUERY_K, final_k=FINAL_K_RRF)\n",
        "    fused_docs = rerank_with_bge(question, docs_before_rerank, top_k=FINAL_K_RERANK)\n",
        "    retriever = StaticRetriever(docs=fused_docs)\n",
        "\n",
        "    rag_chain = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        retriever=retriever,\n",
        "        chain_type=\"stuff\",\n",
        "        chain_type_kwargs={\"prompt\": prompt},\n",
        "        return_source_documents=True\n",
        "    )\n",
        "\n",
        "    result = rag_chain.invoke({\"query\": question})\n",
        "    output_text = result[\"result\"]\n",
        "\n",
        "    retrieved_chunks = [doc.page_content for doc in fused_docs]\n",
        "    evidence_text = \" \".join(retrieved_chunks)  # 一個字串，供模型與 ROUGE 使用\n",
        "\n",
        "    answer_text = \"\"\n",
        "    for line in output_text.splitlines():\n",
        "        if line.lower().startswith(\"answer:\"):\n",
        "            answer_text = line[len(\"answer:\"):].strip()\n",
        "\n",
        "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "    rouge_result = scorer.score(ground_truth_evidence, evidence_text)\n",
        "    rouge_l_score = rouge_result[\"rougeL\"].fmeasure\n",
        "\n",
        "    item = {\n",
        "        \"title\": title,\n",
        "        \"question\": question,\n",
        "        \"predicted_answer\": answer_text,\n",
        "        \"ground_truth_answer\": ground_truth_answer,\n",
        "        \"predicted_evidence\": retrieved_chunks,\n",
        "        \"ground_truth_evidence\": ground_truth_evidence,\n",
        "        \"rougeL\": round(rouge_l_score, 4),\n",
        "\n",
        "    }\n",
        "    results.append(item)\n",
        "\n",
        "    with open(checkpoint_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "\n",
        "for idx, res in enumerate(results[:3]):\n",
        "    print(f\"--- Sample {idx+1} ---\")\n",
        "    print(f\"Title: {res['title']}\")\n",
        "    print(f\"Question: {res['question']}\")\n",
        "    print(f\"Predicted Answer: {res['predicted_answer']}\")\n",
        "    print(f\"Ground Truth Answer: {res['ground_truth_answer']}\")\n",
        "    print(f\"Predicted Evidence: {res['predicted_evidence']}\")\n",
        "    print(f\"Ground Truth Evidence: {res['ground_truth_evidence']}\")\n",
        "    print(f\"ROUGE-L: {res['rougeL']}\")\n",
        "    print(\"-\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KszZ-hKhrUb",
        "outputId": "980b90a9-e08a-4965-99e7-a4b72b5f63c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== 📊 驗證集整體評估結果 ===\n",
            "📈 平均 ROUGE-L 分數：0.1857\n",
            "📈 超多證據：0.0000\n",
            "📈 一半證據：0.0000\n",
            "📈 有提到證據：22.0000\n",
            "📈 平均 ROUGE-L 分數：0.1857\n"
          ]
        }
      ],
      "source": [
        "###all10/15/5\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "\n",
        "rouge_scores = [res[\"rougeL\"] for res in results]\n",
        "\n",
        "avg_rouge = np.mean(rouge_scores)\n",
        "lot_evidence = sum(1 for s in rouge_scores if s >= 0.7)\n",
        "half_evidence = sum(1 for s in rouge_scores if s >= 0.5)\n",
        "enough_evidence = sum(1 for s in rouge_scores if s >= 0.3)\n",
        "\n",
        "print(\"\\n=== 📊 驗證集整體評估結果 ===\")\n",
        "print(f\"📈 平均 ROUGE-L 分數：{avg_rouge:.4f}\")\n",
        "print(f\"📈 超多證據：{lot_evidence:.4f}\")\n",
        "print(f\"📈 一半證據：{half_evidence:.4f}\")\n",
        "print(f\"📈 有提到證據：{enough_evidence:.4f}\")\n",
        "print(f\"📈 平均 ROUGE-L 分數：{avg_rouge:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjbnN9_DxFQw"
      },
      "source": [
        "###### **4.** Answer Hypothesis → RRF → Rerank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "entWtsHbxFQw"
      },
      "outputs": [],
      "source": [
        "def generate_hypothesis_queries(question: str) -> list[str]:\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": (\n",
        "            \"You are a knowledgeable research assistant. \"\n",
        "            \"Based on the research question, hypothesize 2 possible answers (concise statements). \"\n",
        "            \"Only return the 2 hypotheses, one per line, no explanations.\"\n",
        "        )},\n",
        "        {\"role\": \"user\", \"content\": f\"Question: {question}\"}\n",
        "    ]\n",
        "\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        messages, return_tensors=\"pt\", add_generation_prompt=True\n",
        "    ).to(model_judge.device)\n",
        "\n",
        "    output_ids = model_judge.generate(\n",
        "        input_ids=input_ids,\n",
        "        max_new_tokens=80,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        eos_token_id=[tokenizer.eos_token_id],\n",
        "        do_sample=False\n",
        "    )\n",
        "\n",
        "    output_text = tokenizer.decode(output_ids[0][input_ids.shape[-1]:], skip_special_tokens=True)\n",
        "\n",
        "    hypotheses = [question.strip()]\n",
        "    hypotheses += [h.strip(\"-•0123456789. \") for h in output_text.splitlines() if h.strip()]\n",
        "    return hypotheses\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xw5zqECsuOFG"
      },
      "outputs": [],
      "source": [
        "def retrieve_rag_fusion(vector_store, title: str, question: str, per_query_k: int = 10, final_k: int = 20):\n",
        "    queries = generate_hypothesis_queries(question)\n",
        "    all_results = []\n",
        "\n",
        "    for idx, query in enumerate(queries):\n",
        "        docs = retrieve_with_title_fallback(vector_store, title, query, top_k=per_query_k)\n",
        "        all_results.append(docs)\n",
        "        print(f\"  - Query {idx+1}: {query} => {len(docs)} docs\")  # debug info\n",
        "\n",
        "    fused_docs = reciprocal_rank_fusion(all_results)\n",
        "\n",
        "\n",
        "    return fused_docs[:final_k]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "2Jtt-R99nNJ_"
      },
      "outputs": [],
      "source": [
        "PER_QUERY_K = 10\n",
        "FINAL_K_RRF = 15\n",
        "FINAL_K_RERANK = 5\n",
        "answers = []\n",
        "results = []\n",
        "\n",
        "\n",
        "checkpoint_path = \"raghypobge10_15_5all.json\"\n",
        "\n",
        "if os.path.exists(checkpoint_path):\n",
        "    with open(checkpoint_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        results = json.load(f)\n",
        "    done_indices = {r[\"title\"] for r in results}  # 用 title 作為索引（視情況也可加 question）\n",
        "else:\n",
        "    results = []\n",
        "    done_indices = set()\n",
        "\n",
        "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
        "    title = row[\"title\"].strip()\n",
        "    if title in done_indices:\n",
        "        continue  # 跳過已處理過的\n",
        "\n",
        "    full_text = row[\"full_text\"].strip()\n",
        "    question = row[\"question\"].strip()\n",
        "    ground_truth_answer = \" \".join(row[\"answer\"]).strip()\n",
        "    ground_truth_evidence = \" \".join(row[\"evidence\"]).strip()\n",
        "\n",
        "    prompt = build_single_input_prompt(title)\n",
        "\n",
        "    docs_before_rerank = retrieve_rag_fusion(vector_store, title, question, per_query_k=PER_QUERY_K, final_k=FINAL_K_RRF)\n",
        "    fused_docs = rerank_with_bge(question, docs_before_rerank, top_k=FINAL_K_RERANK)\n",
        "    retriever = StaticRetriever(docs=fused_docs)\n",
        "\n",
        "    rag_chain = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        retriever=retriever,\n",
        "        chain_type=\"stuff\",\n",
        "        chain_type_kwargs={\"prompt\": prompt},\n",
        "        return_source_documents=True\n",
        "    )\n",
        "\n",
        "    result = rag_chain.invoke({\"query\": question})\n",
        "    output_text = result[\"result\"]\n",
        "\n",
        "    retrieved_chunks = [doc.page_content for doc in fused_docs]\n",
        "    evidence_text = \" \".join(retrieved_chunks)  # 一個字串，供模型與 ROUGE 使用\n",
        "\n",
        "    answer_text = \"\"\n",
        "    for line in output_text.splitlines():\n",
        "        if line.lower().startswith(\"answer:\"):\n",
        "            answer_text = line[len(\"answer:\"):].strip()\n",
        "\n",
        "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "    rouge_result = scorer.score(ground_truth_evidence, evidence_text)\n",
        "    rouge_l_score = rouge_result[\"rougeL\"].fmeasure\n",
        "\n",
        "    item = {\n",
        "        \"title\": title,\n",
        "        \"question\": question,\n",
        "        \"predicted_answer\": answer_text,\n",
        "        \"ground_truth_answer\": ground_truth_answer,\n",
        "        \"predicted_evidence\": retrieved_chunks,\n",
        "        \"ground_truth_evidence\": ground_truth_evidence,\n",
        "        \"rougeL\": round(rouge_l_score, 4),\n",
        "    }\n",
        "    results.append(item)\n",
        "\n",
        "    with open(checkpoint_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "\n",
        "# === 印出前三筆結果 ===\n",
        "for idx, res in enumerate(results[:3]):\n",
        "    print(f\"--- Sample {idx+1} ---\")\n",
        "    print(f\"Title: {res['title']}\")\n",
        "    print(f\"Question: {res['question']}\")\n",
        "    print(f\"Predicted Answer: {res['predicted_answer']}\")\n",
        "    print(f\"Ground Truth Answer: {res['ground_truth_answer']}\")\n",
        "    print(f\"Predicted Evidence: {res['predicted_evidence']}\")\n",
        "    print(f\"Ground Truth Evidence: {res['ground_truth_evidence']}\")\n",
        "    print(f\"ROUGE-L: {res['rougeL']}\")\n",
        "    print(\"-\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAqXfSJjnW2z",
        "outputId": "df477b13-4b29-47d8-829e-a48969dd12d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== 📊 驗證集整體評估結果 ===\n",
            "📈 平均 ROUGE-L 分數：0.1674\n",
            "📈 超多證據：0.0000\n",
            "📈 一半證據：0.0000\n",
            "📈 有提到證據：16.0000\n",
            "📈 平均 ROUGE-L 分數：0.1674\n"
          ]
        }
      ],
      "source": [
        "#all10-15-5\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "\n",
        "rouge_scores = [res[\"rougeL\"] for res in results]\n",
        "\n",
        "avg_rouge = np.mean(rouge_scores)\n",
        "lot_evidence = sum(1 for s in rouge_scores if s >= 0.7)\n",
        "half_evidence = sum(1 for s in rouge_scores if s >= 0.5)\n",
        "enough_evidence = sum(1 for s in rouge_scores if s >= 0.3)\n",
        "\n",
        "print(\"\\n=== 📊 驗證集整體評估結果 ===\")\n",
        "print(f\"📈 平均 ROUGE-L 分數：{avg_rouge:.4f}\")\n",
        "print(f\"📈 超多證據：{lot_evidence:.4f}\")\n",
        "print(f\"📈 一半證據：{half_evidence:.4f}\")\n",
        "print(f\"📈 有提到證據：{enough_evidence:.4f}\")\n",
        "print(f\"📈 平均 ROUGE-L 分數：{avg_rouge:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZnszau6xFQy"
      },
      "source": [
        "###### **5.** Query Augmentation + Answer Hypothesis → RRF → Rerank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHN5YUFQxFQy"
      },
      "outputs": [],
      "source": [
        "def generate_queries_from_llama(question: str) -> list[str]:\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": (\n",
        "            \"You are a helpful assistant that expands a given research question into 2 additional diverse search queries.\\n\"\n",
        "            \"These queries should cover different perspectives, subtopics, or alternative phrasings of the original question.\\n\"\n",
        "            \"Return exactly 2 queries, one per line, without any explanations or numbering.\"\n",
        "        )},\n",
        "        {\"role\": \"user\", \"content\": f\"Original Question: {question}\\nGenerate 2 related but diverse search queries:\"}\n",
        "    ]\n",
        "\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        messages, return_tensors=\"pt\", add_generation_prompt=True\n",
        "    ).to(model_judge.device)\n",
        "\n",
        "    terminators = [\n",
        "        tokenizer.eos_token_id,\n",
        "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "    ]\n",
        "\n",
        "    output_ids = model_judge.generate(\n",
        "        input_ids=input_ids,\n",
        "        max_new_tokens=64,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        eos_token_id=terminators,\n",
        "        do_sample=False,\n",
        "    )\n",
        "\n",
        "    output_text = tokenizer.decode(output_ids[0][input_ids.shape[-1]:], skip_special_tokens=True)\n",
        "\n",
        "    expanded_queries = [question.strip()]  # 原始查詢第一筆\n",
        "    expanded_queries += [q.strip(\"-•0123456789. \") for q in output_text.splitlines() if q.strip()]\n",
        "    return expanded_queries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SfTBT4ObxFQy"
      },
      "outputs": [],
      "source": [
        "def generate_hypothesis_queries_noorig(question: str) -> list[str]:\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": (\n",
        "            \"You are a knowledgeable research assistant. \"\n",
        "            \"Based on the research question, hypothesize 1 possible answers (concise statements). \"\n",
        "            \"Only return the 1 hypotheses, one per line, no explanations.\"\n",
        "        )},\n",
        "        {\"role\": \"user\", \"content\": f\"Question: {question}\"}\n",
        "    ]\n",
        "\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        messages, return_tensors=\"pt\", add_generation_prompt=True\n",
        "    ).to(model_judge.device)\n",
        "\n",
        "    output_ids = model_judge.generate(\n",
        "        input_ids=input_ids,\n",
        "        max_new_tokens=80,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        eos_token_id=[tokenizer.eos_token_id],\n",
        "        do_sample=False\n",
        "    )\n",
        "\n",
        "    output_text = tokenizer.decode(output_ids[0][input_ids.shape[-1]:], skip_special_tokens=True)\n",
        "\n",
        "    hypotheses = [h.strip(\"-•0123456789. \") for h in output_text.splitlines() if h.strip()]\n",
        "    return hypotheses\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWNJ9NWWxFQz"
      },
      "outputs": [],
      "source": [
        "# === Retrieval based on RAG Fusion ===\n",
        "def retrieve_rag_fusion(vector_store, title: str, question: str, per_query_k: int = 10, final_k: int = 20):\n",
        "    queries = generate_queries_from_llama(question) + generate_hypothesis_queries_noorig(question)\n",
        "\n",
        "    all_results = []\n",
        "\n",
        "    for idx, query in enumerate(queries):\n",
        "        docs = retrieve_with_title_fallback(vector_store, title, query, top_k=per_query_k)\n",
        "        all_results.append(docs)\n",
        "        print(f\"  - Query {idx+1}: {query} => {len(docs)} docs\")  # debug info\n",
        "\n",
        "    fused_docs = reciprocal_rank_fusion(all_results)\n",
        "\n",
        "    print(\"\\n🔁 RRF Top Documents:\")\n",
        "    for i, doc in enumerate(fused_docs[:final_k]):\n",
        "        print(f\"  🔁 RRF #{i+1} | Title: {doc.metadata.get('title')} | Preview: {doc.page_content[:200]}...\")\n",
        "\n",
        "    return fused_docs[:final_k]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "iKjO1wOHqZla"
      },
      "outputs": [],
      "source": [
        "PER_QUERY_K = 10\n",
        "FINAL_K_RRF = 15\n",
        "FINAL_K_RERANK = 5\n",
        "answers = []\n",
        "results = []\n",
        "\n",
        "\n",
        "checkpoint_path = \"raghypoquerybge10_15_5all.json\"\n",
        "\n",
        "if os.path.exists(checkpoint_path):\n",
        "    with open(checkpoint_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        results = json.load(f)\n",
        "    done_indices = {r[\"title\"] for r in results}  # 用 title 作為索引（視情況也可加 question）\n",
        "else:\n",
        "    results = []\n",
        "    done_indices = set()\n",
        "\n",
        "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
        "    title = row[\"title\"].strip()\n",
        "    if title in done_indices:\n",
        "        continue\n",
        "\n",
        "    full_text = row[\"full_text\"].strip()\n",
        "    question = row[\"question\"].strip()\n",
        "    ground_truth_answer = \" \".join(row[\"answer\"]).strip()\n",
        "    ground_truth_evidence = \" \".join(row[\"evidence\"]).strip()\n",
        "\n",
        "    prompt = build_single_input_prompt(title)\n",
        "\n",
        "    docs_before_rerank = retrieve_rag_fusion(vector_store, title, question, per_query_k=PER_QUERY_K, final_k=FINAL_K_RRF)\n",
        "    fused_docs = rerank_with_bge(question, docs_before_rerank, top_k=FINAL_K_RERANK)\n",
        "    retriever = StaticRetriever(docs=fused_docs)\n",
        "\n",
        "    rag_chain = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        retriever=retriever,\n",
        "        chain_type=\"stuff\",\n",
        "        chain_type_kwargs={\"prompt\": prompt},\n",
        "        return_source_documents=True\n",
        "    )\n",
        "\n",
        "    result = rag_chain.invoke({\"query\": question})\n",
        "    output_text = result[\"result\"]\n",
        "\n",
        "    retrieved_chunks = [doc.page_content for doc in fused_docs]\n",
        "    evidence_text = \" \".join(retrieved_chunks)\n",
        "\n",
        "    answer_text = \"\"\n",
        "    for line in output_text.splitlines():\n",
        "        if line.lower().startswith(\"answer:\"):\n",
        "            answer_text = line[len(\"answer:\"):].strip()\n",
        "\n",
        "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "    rouge_result = scorer.score(ground_truth_evidence, evidence_text)\n",
        "    rouge_l_score = rouge_result[\"rougeL\"].fmeasure\n",
        "\n",
        "    item = {\n",
        "        \"title\": title,\n",
        "        \"question\": question,\n",
        "        \"predicted_answer\": answer_text,\n",
        "        \"ground_truth_answer\": ground_truth_answer,\n",
        "        \"predicted_evidence\": retrieved_chunks,\n",
        "        \"ground_truth_evidence\": ground_truth_evidence,\n",
        "        \"rougeL\": round(rouge_l_score, 4),\n",
        "    }\n",
        "    results.append(item)\n",
        "\n",
        "    # === ✅ 寫入 checkpoint（每筆一存）===\n",
        "    with open(checkpoint_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "\n",
        "\n",
        "# === 印出前三筆結果 ===\n",
        "for idx, res in enumerate(results[:3]):\n",
        "    print(f\"--- Sample {idx+1} ---\")\n",
        "    print(f\"Title: {res['title']}\")\n",
        "    print(f\"Question: {res['question']}\")\n",
        "    print(f\"Predicted Answer: {res['predicted_answer']}\")\n",
        "    print(f\"Ground Truth Answer: {res['ground_truth_answer']}\")\n",
        "    print(f\"Predicted Evidence: {res['predicted_evidence']}\")\n",
        "    print(f\"Ground Truth Evidence: {res['ground_truth_evidence']}\")\n",
        "    print(f\"ROUGE-L: {res['rougeL']}\")\n",
        "    print(\"-\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4M0xfT2DqjFc",
        "outputId": "ca54693e-3b7e-4c15-e1e6-90df8b030b46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== 📊 驗證集整體評估結果 ===\n",
            "📈 平均 ROUGE-L 分數：0.1831\n",
            "📈 超多證據：0.0000\n",
            "📈 一半證據：0.0000\n",
            "📈 有提到證據：20.0000\n"
          ]
        }
      ],
      "source": [
        "#10-15-5\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "\n",
        "rouge_scores = [res[\"rougeL\"] for res in results]\n",
        "\n",
        "avg_rouge = np.mean(rouge_scores)\n",
        "lot_evidence = sum(1 for s in rouge_scores if s >= 0.7)\n",
        "half_evidence = sum(1 for s in rouge_scores if s >= 0.5)\n",
        "enough_evidence = sum(1 for s in rouge_scores if s >= 0.3)\n",
        "\n",
        "print(\"\\n=== 📊 驗證集整體評估結果 ===\")\n",
        "print(f\"📈 平均 ROUGE-L 分數：{avg_rouge:.4f}\")\n",
        "print(f\"📈 超多證據：{lot_evidence:.4f}\")\n",
        "print(f\"📈 一半證據：{half_evidence:.4f}\")\n",
        "print(f\"📈 有提到證據：{enough_evidence:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gzk9fJQ6sGfw"
      },
      "source": [
        "##### 4. Step4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfxC7GCEHowI"
      },
      "source": [
        "###### **6.**Query Augmentation → RRF → Rerank → Score Filter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NwKkFnRWHowJ"
      },
      "outputs": [],
      "source": [
        "def generate_queries_from_llama(question: str) -> list[str]:\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": (\n",
        "            \"You are a helpful assistant that expands a given research question into 2 additional diverse search queries.\\n\"\n",
        "            \"These queries should cover different perspectives, subtopics, or alternative phrasings of the original question.\\n\"\n",
        "            \"Return exactly 2 queries, one per line, without any explanations or numbering.\"\n",
        "        )},\n",
        "        {\"role\": \"user\", \"content\": f\"Original Question: {question}\\nGenerate 2 related but diverse search queries:\"}\n",
        "    ]\n",
        "\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        messages, return_tensors=\"pt\", add_generation_prompt=True\n",
        "    ).to(model_judge.device)\n",
        "\n",
        "    terminators = [\n",
        "        tokenizer.eos_token_id,\n",
        "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "    ]\n",
        "\n",
        "    output_ids = model_judge.generate(\n",
        "        input_ids=input_ids,\n",
        "        max_new_tokens=64,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        eos_token_id=terminators,\n",
        "        do_sample=False,\n",
        "    )\n",
        "\n",
        "    output_text = tokenizer.decode(output_ids[0][input_ids.shape[-1]:], skip_special_tokens=True)\n",
        "\n",
        "    expanded_queries = [question.strip()]\n",
        "    expanded_queries += [q.strip(\"-•0123456789. \") for q in output_text.splitlines() if q.strip()]\n",
        "    return expanded_queries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2XomvcnuHowK"
      },
      "outputs": [],
      "source": [
        "def rerank_with_bge_standard(query: str, docs: list[Document], top_k: int = 8, verbose=False):\n",
        "    pairs = [[query, doc.page_content] for doc in docs]\n",
        "    scores = reranker.predict(pairs)\n",
        "    reranked = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    filtered = [(doc, score) for doc, score in reranked if score > 0.6]\n",
        "\n",
        "    if verbose:\n",
        "        print(\"\\n🔁 Rerank Result:\")\n",
        "        for rank, (doc, score) in enumerate(filtered if filtered else reranked[:top_k]):\n",
        "            print(f\"\\n# {rank+1} | Score: {score:.4f} | Title: {doc.metadata.get('title', '[No title]')}\")\n",
        "            print(f\"Chunk ID: {doc.metadata.get('chunk_id', '[No ID]')}\")\n",
        "            print(f\"Content Preview:\\n{doc.page_content[:300]}...\")\n",
        "\n",
        "    return [doc for doc, _ in (filtered if filtered else reranked[:top_k])]\n",
        "\n",
        "# === Retrieval based on RAG Fusion ===\n",
        "def retrieve_rag_fusion(vector_store, title: str, question: str, per_query_k: int = 10, final_k: int = 20):\n",
        "    queries = generate_queries_from_llama(question)\n",
        "    all_results = []\n",
        "\n",
        "    for idx, query in enumerate(queries):\n",
        "        docs = retrieve_with_title_fallback(vector_store, title, query, top_k=per_query_k)\n",
        "        all_results.append(docs)\n",
        "        print(f\"  - Query {idx+1}: {query} => {len(docs)} docs\")  # debug info\n",
        "\n",
        "\n",
        "    fused_docs = reciprocal_rank_fusion(all_results)\n",
        "\n",
        "\n",
        "    print(\"\\n🔁 RRF Top Documents:\")\n",
        "    for i, doc in enumerate(fused_docs[:final_k]):\n",
        "        print(f\"  🔁 RRF #{i+1} | Title: {doc.metadata.get('title')} | Preview: {doc.page_content[:200]}...\")\n",
        "\n",
        "    return fused_docs[:final_k]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ibS0Cfd3VdYC"
      },
      "outputs": [],
      "source": [
        "PER_QUERY_K = 10\n",
        "FINAL_K_RRF = 15\n",
        "FINAL_K_RERANK = 5\n",
        "answers = []\n",
        "results = []\n",
        "\n",
        "\n",
        "checkpoint_path = \"ragfusionbge0.610_15_5all.json\"\n",
        "\n",
        "if os.path.exists(checkpoint_path):\n",
        "    with open(checkpoint_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        results = json.load(f)\n",
        "    done_indices = {r[\"title\"] for r in results}  # 用 title 作為索引（視情況也可加 question）\n",
        "else:\n",
        "    results = []\n",
        "    done_indices = set()\n",
        "\n",
        "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
        "    title = row[\"title\"].strip()\n",
        "    if title in done_indices:\n",
        "        continue  # 跳過已處理過的\n",
        "\n",
        "    full_text = row[\"full_text\"].strip()\n",
        "    question = row[\"question\"].strip()\n",
        "    ground_truth_answer = \" \".join(row[\"answer\"]).strip()\n",
        "    ground_truth_evidence = \" \".join(row[\"evidence\"]).strip()\n",
        "\n",
        "    prompt = build_single_input_prompt(title)\n",
        "\n",
        "    docs_before_rerank = retrieve_rag_fusion(vector_store, title, question, per_query_k=PER_QUERY_K, final_k=FINAL_K_RRF)\n",
        "    fused_docs = rerank_with_bge_standard(question, docs_before_rerank, top_k=FINAL_K_RERANK)\n",
        "    retriever = StaticRetriever(docs=fused_docs)\n",
        "\n",
        "    rag_chain = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        retriever=retriever,\n",
        "        chain_type=\"stuff\",\n",
        "        chain_type_kwargs={\"prompt\": prompt},\n",
        "        return_source_documents=True\n",
        "    )\n",
        "\n",
        "    result = rag_chain.invoke({\"query\": question})\n",
        "    output_text = result[\"result\"]\n",
        "\n",
        "    retrieved_chunks = [doc.page_content for doc in fused_docs]\n",
        "    evidence_text = \" \".join(retrieved_chunks)\n",
        "\n",
        "    answer_text = \"\"\n",
        "    for line in output_text.splitlines():\n",
        "        if line.lower().startswith(\"answer:\"):\n",
        "            answer_text = line[len(\"answer:\"):].strip()\n",
        "\n",
        "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "    rouge_result = scorer.score(ground_truth_evidence, evidence_text)\n",
        "    rouge_l_score = rouge_result[\"rougeL\"].fmeasure\n",
        "\n",
        "    item = {\n",
        "        \"title\": title,\n",
        "        \"question\": question,\n",
        "        \"predicted_answer\": answer_text,\n",
        "        \"ground_truth_answer\": ground_truth_answer,\n",
        "        \"predicted_evidence\": retrieved_chunks,\n",
        "        \"ground_truth_evidence\": ground_truth_evidence,\n",
        "        \"rougeL\": round(rouge_l_score, 4),\n",
        "\n",
        "    }\n",
        "    results.append(item)\n",
        "\n",
        "    # === ✅ 寫入 checkpoint（每筆一存）===\n",
        "    with open(checkpoint_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "\n",
        "# === 印出前三筆結果 ===\n",
        "for idx, res in enumerate(results[:3]):\n",
        "    print(f\"--- Sample {idx+1} ---\")\n",
        "    print(f\"Title: {res['title']}\")\n",
        "    print(f\"Question: {res['question']}\")\n",
        "    print(f\"Predicted Answer: {res['predicted_answer']}\")\n",
        "    print(f\"Ground Truth Answer: {res['ground_truth_answer']}\")\n",
        "    print(f\"Predicted Evidence: {res['predicted_evidence']}\")\n",
        "    print(f\"Ground Truth Evidence: {res['ground_truth_evidence']}\")\n",
        "    print(f\"ROUGE-L: {res['rougeL']}\")\n",
        "    print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEtemr0_cnFp",
        "outputId": "ca78268b-5e1d-46a3-d747-98e37f08af15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== 📊 驗證集整體評估結果 ===\n",
            "📈 平均 ROUGE-L 分數：0.2043\n",
            "📈 超多證據：3.0000\n",
            "📈 一半證據：5.0000\n",
            "📈 有提到證據：23.0000\n",
            "📈 平均 ROUGE-L 分數：0.2043\n"
          ]
        }
      ],
      "source": [
        "####all10-15-5\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "\n",
        "rouge_scores = [res[\"rougeL\"] for res in results]\n",
        "\n",
        "avg_rouge = np.mean(rouge_scores)\n",
        "lot_evidence = sum(1 for s in rouge_scores if s >= 0.7)\n",
        "half_evidence = sum(1 for s in rouge_scores if s >= 0.5)\n",
        "enough_evidence = sum(1 for s in rouge_scores if s >= 0.3)\n",
        "\n",
        "print(\"\\n=== 📊 驗證集整體評估結果 ===\")\n",
        "print(f\"📈 平均 ROUGE-L 分數：{avg_rouge:.4f}\")\n",
        "print(f\"📈 超多證據：{lot_evidence:.4f}\")\n",
        "print(f\"📈 一半證據：{half_evidence:.4f}\")\n",
        "print(f\"📈 有提到證據：{enough_evidence:.4f}\")\n",
        "print(f\"📈 平均 ROUGE-L 分數：{avg_rouge:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78dI_VzjnDFf"
      },
      "source": [
        "###### **7.** Query Augmentation+Answer → RRF → Rerank → Score Filter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBrJevJlnDFg"
      },
      "outputs": [],
      "source": [
        "def generate_queries_from_llama(question: str) -> list[str]:\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": (\n",
        "            \"You are a helpful assistant that expands a given research question into 2 additional diverse search queries.\\n\"\n",
        "            \"These queries should cover different perspectives, subtopics, or alternative phrasings of the original question.\\n\"\n",
        "            \"Return exactly 2 queries, one per line, without any explanations or numbering.\"\n",
        "        )},\n",
        "        {\"role\": \"user\", \"content\": f\"Original Question: {question}\\nGenerate 2 related but diverse search queries:\"}\n",
        "    ]\n",
        "\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        messages, return_tensors=\"pt\", add_generation_prompt=True\n",
        "    ).to(model_judge.device)\n",
        "\n",
        "    terminators = [\n",
        "        tokenizer.eos_token_id,\n",
        "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "    ]\n",
        "\n",
        "    output_ids = model_judge.generate(\n",
        "        input_ids=input_ids,\n",
        "        max_new_tokens=64,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        eos_token_id=terminators,\n",
        "        do_sample=False,\n",
        "    )\n",
        "\n",
        "    output_text = tokenizer.decode(output_ids[0][input_ids.shape[-1]:], skip_special_tokens=True)\n",
        "\n",
        "    expanded_queries = [question.strip()]\n",
        "    expanded_queries += [q.strip(\"-•0123456789. \") for q in output_text.splitlines() if q.strip()]\n",
        "    return expanded_queries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnKYAfnhnDFg"
      },
      "outputs": [],
      "source": [
        "def generate_hypothesis_queries_noorig(question: str) -> list[str]:\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": (\n",
        "            \"You are a knowledgeable research assistant. \"\n",
        "            \"Based on the research question, hypothesize 1 possible answers (concise statements). \"\n",
        "            \"Only return the 1 hypotheses, one per line, no explanations.\"\n",
        "        )},\n",
        "        {\"role\": \"user\", \"content\": f\"Question: {question}\"}\n",
        "    ]\n",
        "\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        messages, return_tensors=\"pt\", add_generation_prompt=True\n",
        "    ).to(model_judge.device)\n",
        "\n",
        "    output_ids = model_judge.generate(\n",
        "        input_ids=input_ids,\n",
        "        max_new_tokens=80,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        eos_token_id=[tokenizer.eos_token_id],\n",
        "        do_sample=False\n",
        "    )\n",
        "\n",
        "    output_text = tokenizer.decode(output_ids[0][input_ids.shape[-1]:], skip_special_tokens=True)\n",
        "\n",
        "    hypotheses = [h.strip(\"-•0123456789. \") for h in output_text.splitlines() if h.strip()]\n",
        "    return hypotheses\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUJdx0qKnDFh"
      },
      "outputs": [],
      "source": [
        "def rerank_with_bge_standard(query: str, docs: list[Document], top_k: int = 8, verbose=False):\n",
        "    pairs = [[query, doc.page_content] for doc in docs]\n",
        "    scores = reranker.predict(pairs)\n",
        "    reranked = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    filtered = [(doc, score) for doc, score in reranked if score > 0.6]\n",
        "\n",
        "    if verbose:\n",
        "        print(\"\\n🔁 Rerank Result:\")\n",
        "        for rank, (doc, score) in enumerate(filtered if filtered else reranked[:top_k]):\n",
        "            print(f\"\\n# {rank+1} | Score: {score:.4f} | Title: {doc.metadata.get('title', '[No title]')}\")\n",
        "            print(f\"Chunk ID: {doc.metadata.get('chunk_id', '[No ID]')}\")\n",
        "            print(f\"Content Preview:\\n{doc.page_content[:300]}...\")\n",
        "\n",
        "    return [doc for doc, _ in (filtered if filtered else reranked[:top_k])]\n",
        "\n",
        "# === Retrieval based on RAG Fusion ===\n",
        "def retrieve_rag_fusion(vector_store, title: str, question: str, per_query_k: int = 10, final_k: int = 20):\n",
        "    queries = generate_queries_from_llama(question) + generate_hypothesis_queries_noorig(question)\n",
        "\n",
        "    all_results = []\n",
        "\n",
        "    for idx, query in enumerate(queries):\n",
        "        docs = retrieve_with_title_fallback(vector_store, title, query, top_k=per_query_k)\n",
        "        all_results.append(docs)\n",
        "        print(f\"  - Query {idx+1}: {query} => {len(docs)} docs\")  # debug info\n",
        "\n",
        "    fused_docs = reciprocal_rank_fusion(all_results)\n",
        "\n",
        "    print(\"\\n🔁 RRF Top Documents:\")\n",
        "    for i, doc in enumerate(fused_docs[:final_k]):\n",
        "        print(f\"  🔁 RRF #{i+1} | Title: {doc.metadata.get('title')} | Preview: {doc.page_content[:200]}...\")\n",
        "\n",
        "    return fused_docs[:final_k]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "IX2x4SEGVv_Y"
      },
      "outputs": [],
      "source": [
        "PER_QUERY_K = 10\n",
        "FINAL_K_RRF = 15\n",
        "FINAL_K_RERANK = 5\n",
        "answers = []\n",
        "results = []\n",
        "\n",
        "\n",
        "checkpoint_path = \"raghypoquerybgestd0.610_15_5all.json\"\n",
        "\n",
        "if os.path.exists(checkpoint_path):\n",
        "    with open(checkpoint_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        results = json.load(f)\n",
        "    done_indices = {r[\"title\"] for r in results}\n",
        "else:\n",
        "    results = []\n",
        "    done_indices = set()\n",
        "\n",
        "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
        "    title = row[\"title\"].strip()\n",
        "    if title in done_indices:\n",
        "        continue\n",
        "\n",
        "    full_text = row[\"full_text\"].strip()\n",
        "    question = row[\"question\"].strip()\n",
        "    ground_truth_answer = \" \".join(row[\"answer\"]).strip()\n",
        "    ground_truth_evidence = \" \".join(row[\"evidence\"]).strip()\n",
        "\n",
        "    prompt = build_single_input_prompt(title)\n",
        "\n",
        "    docs_before_rerank = retrieve_rag_fusion(vector_store, title, question, per_query_k=PER_QUERY_K, final_k=FINAL_K_RRF)\n",
        "    fused_docs = rerank_with_bge_standard(question, docs_before_rerank, top_k=FINAL_K_RERANK)\n",
        "    retriever = StaticRetriever(docs=fused_docs)\n",
        "\n",
        "    rag_chain = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        retriever=retriever,\n",
        "        chain_type=\"stuff\",\n",
        "        chain_type_kwargs={\"prompt\": prompt},\n",
        "        return_source_documents=True\n",
        "    )\n",
        "\n",
        "    result = rag_chain.invoke({\"query\": question})\n",
        "    output_text = result[\"result\"]\n",
        "\n",
        "    retrieved_chunks = [doc.page_content for doc in fused_docs]\n",
        "    evidence_text = \" \".join(retrieved_chunks)\n",
        "\n",
        "    answer_text = \"\"\n",
        "    for line in output_text.splitlines():\n",
        "        if line.lower().startswith(\"answer:\"):\n",
        "            answer_text = line[len(\"answer:\"):].strip()\n",
        "\n",
        "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "    rouge_result = scorer.score(ground_truth_evidence, evidence_text)\n",
        "    rouge_l_score = rouge_result[\"rougeL\"].fmeasure\n",
        "\n",
        "    item = {\n",
        "        \"title\": title,\n",
        "        \"question\": question,\n",
        "        \"predicted_answer\": answer_text,\n",
        "        \"ground_truth_answer\": ground_truth_answer,\n",
        "        \"predicted_evidence\": retrieved_chunks,\n",
        "        \"ground_truth_evidence\": ground_truth_evidence,\n",
        "        \"rougeL\": round(rouge_l_score, 4),\n",
        "    }\n",
        "    results.append(item)\n",
        "\n",
        "    with open(checkpoint_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "\n",
        "# === 印出前三筆結果 ===\n",
        "for idx, res in enumerate(results[:3]):\n",
        "    print(f\"--- Sample {idx+1} ---\")\n",
        "    print(f\"Title: {res['title']}\")\n",
        "    print(f\"Question: {res['question']}\")\n",
        "    print(f\"Predicted Answer: {res['predicted_answer']}\")\n",
        "    print(f\"Ground Truth Answer: {res['ground_truth_answer']}\")\n",
        "    print(f\"Predicted Evidence: {res['predicted_evidence']}\")\n",
        "    print(f\"Ground Truth Evidence: {res['ground_truth_evidence']}\")\n",
        "    print(f\"ROUGE-L: {res['rougeL']}\")\n",
        "    print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvZZ2iCSoX2A",
        "outputId": "f98b450f-6c5a-4991-80d3-c6803fbfe4ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== 📊 驗證集整體評估結果 ===\n",
            "📈 平均 ROUGE-L 分數：0.2017\n",
            "📈 超多證據：3.0000\n",
            "📈 一半證據：4.0000\n",
            "📈 有提到證據：22.0000\n"
          ]
        }
      ],
      "source": [
        "##########all 10-15-5\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "\n",
        "rouge_scores = [res[\"rougeL\"] for res in results]\n",
        "\n",
        "avg_rouge = np.mean(rouge_scores)\n",
        "lot_evidence = sum(1 for s in rouge_scores if s >= 0.7)\n",
        "half_evidence = sum(1 for s in rouge_scores if s >= 0.5)\n",
        "enough_evidence = sum(1 for s in rouge_scores if s >= 0.3)\n",
        "\n",
        "print(\"\\n=== 📊 驗證集整體評估結果 ===\")\n",
        "print(f\"📈 平均 ROUGE-L 分數：{avg_rouge:.4f}\")\n",
        "print(f\"📈 超多證據：{lot_evidence:.4f}\")\n",
        "print(f\"📈 一半證據：{half_evidence:.4f}\")\n",
        "print(f\"📈 有提到證據：{enough_evidence:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYeW4-7-sJrL"
      },
      "source": [
        "##### 5. Step 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xD7IQ0P_3x3"
      },
      "source": [
        "###### **8.**Query Expansion → RRF Fusion → Temp Answer → Answer-Guided Rerank → Score Filter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0u-_-Cdk_3x5"
      },
      "outputs": [],
      "source": [
        "def generate_queries_from_llama(question: str) -> list[str]:\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": (\n",
        "            \"You are a helpful assistant that expands a given research question into 1 additional diverse search queries.\\n\"\n",
        "            \"These queries should cover different perspectives, subtopics, or alternative phrasings of the original question.\\n\"\n",
        "            \"Return exactly 1 queries, one per line, without any explanations or numbering.\"\n",
        "        )},\n",
        "        {\"role\": \"user\", \"content\": f\"Original Question: {question}\\nGenerate 1 related but diverse search queries:\"}\n",
        "    ]\n",
        "\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        messages, return_tensors=\"pt\", add_generation_prompt=True\n",
        "    ).to(model_judge.device)\n",
        "\n",
        "    terminators = [\n",
        "        tokenizer.eos_token_id,\n",
        "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "    ]\n",
        "\n",
        "    output_ids = model_judge.generate(\n",
        "        input_ids=input_ids,\n",
        "        max_new_tokens=64,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        eos_token_id=terminators,\n",
        "        do_sample=False,\n",
        "    )\n",
        "\n",
        "    output_text = tokenizer.decode(output_ids[0][input_ids.shape[-1]:], skip_special_tokens=True)\n",
        "\n",
        "    expanded_queries = [question.strip()]\n",
        "    expanded_queries += [q.strip(\"-•0123456789. \") for q in output_text.splitlines() if q.strip()]\n",
        "    return expanded_queries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XT4viWCb_3x6"
      },
      "outputs": [],
      "source": [
        "def rerank_with_answer_based_bge(answer: str, docs: list[Document], top_k: int = 5, threshold: float = 0.6, verbose=True):\n",
        "    pairs = [[answer, doc.page_content] for doc in docs]\n",
        "    scores = reranker.predict(pairs)\n",
        "    reranked = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    if verbose:\n",
        "        print(\"\\n📌 Rerank by Answer Similarity:\")\n",
        "        for i, (doc, score) in enumerate(reranked[:top_k]):\n",
        "            print(f\"# {i+1} | Score: {score:.4f} | Preview: {doc.page_content[:200]}...\")\n",
        "\n",
        "    return [doc for doc, score in reranked[:top_k] if score >= threshold] or [doc for doc, _ in reranked[:top_k]]\n",
        "\n",
        "\n",
        "# === Retrieval based on RAG Fusion ===\n",
        "def retrieve_rag_fusion(vector_store, title: str, question: str, per_query_k: int = 10, final_k: int = 20):\n",
        "    queries = generate_queries_from_llama(question)\n",
        "\n",
        "    all_results = []\n",
        "\n",
        "    for idx, query in enumerate(queries):\n",
        "        docs = retrieve_with_title_fallback(vector_store, title, query, top_k=per_query_k)\n",
        "        all_results.append(docs)\n",
        "        print(f\"  - Query {idx+1}: {query} => {len(docs)} docs\")  # debug info\n",
        "\n",
        "    fused_docs = reciprocal_rank_fusion(all_results)\n",
        "\n",
        "    print(\"\\n🔁 RRF Top Documents:\")\n",
        "    for i, doc in enumerate(fused_docs[:final_k]):\n",
        "        print(f\"  🔁 RRF #{i+1} | Title: {doc.metadata.get('title')} | Preview: {doc.page_content[:200]}...\")\n",
        "\n",
        "    return fused_docs[:final_k]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Xc3ng2f2Tro-"
      },
      "outputs": [],
      "source": [
        "PER_QUERY_K = 10\n",
        "FINAL_K_RRF = 15\n",
        "FINAL_K_RERANK = 5\n",
        "answers = []\n",
        "results = []\n",
        "\n",
        "\n",
        "checkpoint_path = \"ragquerybgeanswerstd0.610_15_5all.json\"\n",
        "\n",
        "if os.path.exists(checkpoint_path):\n",
        "    with open(checkpoint_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        results = json.load(f)\n",
        "    done_indices = {r[\"title\"] for r in results}  # 用 title 作為索引（視情況也可加 question）\n",
        "else:\n",
        "    results = []\n",
        "    done_indices = set()\n",
        "\n",
        "for i, row in tqdm((df).iterrows(), total=len(df)):\n",
        "    title = row[\"title\"].strip()\n",
        "    if title in done_indices:\n",
        "        continue\n",
        "\n",
        "    full_text = row[\"full_text\"].strip()\n",
        "    question = row[\"question\"].strip()\n",
        "    ground_truth_answer = \" \".join(row[\"answer\"]).strip()\n",
        "    ground_truth_evidence = \" \".join(row[\"evidence\"]).strip()\n",
        "\n",
        "    prompt = build_single_input_prompt(title)\n",
        "\n",
        "    docs_before_rerank = retrieve_rag_fusion(\n",
        "        vector_store, title, question,\n",
        "        per_query_k=PER_QUERY_K, final_k=FINAL_K_RRF\n",
        "    )\n",
        "\n",
        "    temp_retriever = StaticRetriever(docs=docs_before_rerank)\n",
        "    temp_chain = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        retriever=temp_retriever,\n",
        "        chain_type=\"stuff\",\n",
        "        chain_type_kwargs={\"prompt\": prompt},\n",
        "        return_source_documents=True\n",
        "    )\n",
        "    temp_result = temp_chain.invoke({\"query\": question})\n",
        "    output_text = temp_result[\"result\"]\n",
        "\n",
        "    predicted_answer = \"\"\n",
        "    for line in output_text.splitlines():\n",
        "        if line.lower().startswith(\"answer:\"):\n",
        "            predicted_answer = line[len(\"answer:\"):].strip()\n",
        "\n",
        "    fused_docs = rerank_with_answer_based_bge(predicted_answer, docs_before_rerank, top_k=FINAL_K_RERANK)\n",
        "    final_retriever = StaticRetriever(docs=fused_docs)\n",
        "\n",
        "    rag_chain = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        retriever=final_retriever,\n",
        "        chain_type=\"stuff\",\n",
        "        chain_type_kwargs={\"prompt\": prompt},\n",
        "        return_source_documents=True\n",
        "    )\n",
        "    result = rag_chain.invoke({\"query\": question})\n",
        "    answer_text = \"\"\n",
        "    for line in result[\"result\"].splitlines():\n",
        "        if line.lower().startswith(\"answer:\"):\n",
        "            answer_text = line[len(\"answer:\"):].strip()\n",
        "\n",
        "    retrieved_chunks = [doc.page_content for doc in fused_docs]\n",
        "    evidence_text = \" \".join(retrieved_chunks)\n",
        "\n",
        "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "    rouge_result = scorer.score(ground_truth_evidence, evidence_text)\n",
        "    rouge_l_score = rouge_result[\"rougeL\"].fmeasure\n",
        "\n",
        "    item = {\n",
        "        \"title\": title,\n",
        "        \"question\": question,\n",
        "        \"predicted_answer\": answer_text,\n",
        "        \"ground_truth_answer\": ground_truth_answer,\n",
        "        \"predicted_evidence\": retrieved_chunks,\n",
        "        \"ground_truth_evidence\": ground_truth_evidence,\n",
        "        \"rougeL\": round(rouge_l_score, 4),\n",
        "    }\n",
        "    results.append(item)\n",
        "\n",
        "    with open(checkpoint_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "\n",
        "for idx, res in enumerate(results[:3]):\n",
        "    print(f\"--- Sample {idx+1} ---\")\n",
        "    print(f\"Title: {res['title']}\")\n",
        "    print(f\"Question: {res['question']}\")\n",
        "    print(f\"Predicted Answer: {res['predicted_answer']}\")\n",
        "    print(f\"Ground Truth Answer: {res['ground_truth_answer']}\")\n",
        "    print(f\"Predicted Evidence: {res['predicted_evidence']}\")\n",
        "    print(f\"Ground Truth Evidence: {res['ground_truth_evidence']}\")\n",
        "    print(f\"ROUGE-L: {res['rougeL']}\")\n",
        "    print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YcNHzk1iT1T3",
        "outputId": "8d468a05-a097-40eb-f4e8-0637ed2d67bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== 📊 驗證集整體評估結果 ===\n",
            "📈 平均 ROUGE-L 分數：0.2809\n",
            "📈 超多證據：7.0000\n",
            "📈 一半證據：18.0000\n",
            "📈 有提到證據：33.0000\n"
          ]
        }
      ],
      "source": [
        "#all10-15-5\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "\n",
        "rouge_scores = [res[\"rougeL\"] for res in results]\n",
        "\n",
        "avg_rouge = np.mean(rouge_scores)\n",
        "lot_evidence = sum(1 for s in rouge_scores if s >= 0.7)\n",
        "half_evidence = sum(1 for s in rouge_scores if s >= 0.5)\n",
        "enough_evidence = sum(1 for s in rouge_scores if s >= 0.3)\n",
        "\n",
        "print(\"\\n=== 📊 驗證集整體評估結果 ===\")\n",
        "print(f\"📈 平均 ROUGE-L 分數：{avg_rouge:.4f}\")\n",
        "print(f\"📈 超多證據：{lot_evidence:.4f}\")\n",
        "print(f\"📈 一半證據：{half_evidence:.4f}\")\n",
        "print(f\"📈 有提到證據：{enough_evidence:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
